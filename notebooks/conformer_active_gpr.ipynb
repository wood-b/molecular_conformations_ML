{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/li_place/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning of molecular conformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an idealized example of how active learning can be used to predict properties of molecules and polymers and guide experiments or computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick summary of the method\n",
    "- Load initial conformer data file\n",
    "- Train Gaussian process regression (GPR) model on initial data\n",
    "- Load next conformer data file\n",
    "- Make a prediction on the new data\n",
    "- Select new conformers, based on their uncertainty, to have their energy calculated and incorporated in the training set (In reality, I already calculated the engeries using UFF)\n",
    "- Continue this loop until you get to a metric or uncertainly threshold of your choosing, or you hit a wall with your model :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy values are in kcal/mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are to help keep the active learning loop clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    # load and featurize (CoulombMatrixEig) molecules from sdf file using deepchem  \n",
    "    task = ['energy']\n",
    "    smiles_field = \"smiles\"\n",
    "    mol_field = \"mol\"\n",
    "    featurizer_func = dc.feat.coulomb_matrices.CoulombMatrixEig(max_atoms=20, remove_hydrogens=False)\n",
    "    loader = dc.data.SDFLoader(tasks=task, featurizer=featurizer_func, \n",
    "                               smiles_field=\"smiles\", mol_field=\"mol\")\n",
    "    dataset = loader.featurize(file)\n",
    "    return dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(file, seed=None):\n",
    "    # load, featurize molecules from sdf file, and split using deepchem \n",
    "    task = ['energy']\n",
    "    smiles_field = \"smiles\"\n",
    "    mol_field = \"mol\"\n",
    "    featurizer_func = dc.feat.coulomb_matrices.CoulombMatrixEig(max_atoms=20, remove_hydrogens=False)\n",
    "    loader = dc.data.SDFLoader(tasks=task, featurizer=featurizer_func, \n",
    "                               smiles_field=\"smiles\", mol_field=\"mol\")\n",
    "    dataset = loader.featurize(file)\n",
    "    # randomly split data \n",
    "    # not optimizing hyperparameters so no validation split\n",
    "    random_splitter = dc.splits.RandomSplitter()\n",
    "    train_dataset, test_dataset = random_splitter.train_test_split(dataset, seed=seed)\n",
    "    return train_dataset.X, train_dataset.y, test_dataset.X, test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    # predict and get train rmse ... should be very small\n",
    "    y_train_pred, y_train_pred_std = model.predict(X_train, return_std=True)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    # predict and get test rmse\n",
    "    y_pred, y_pred_std = model.predict(X_test, return_std=True)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return np.round(train_rmse, decimals=2), np.round(test_rmse, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_select(model, X_new_data, select_frac):\n",
    "    # predict\n",
    "    y_pred, y_pred_std = model.predict(X_new_data, return_std=True)\n",
    "    # find the conformers with the highest prediction uncertainty\n",
    "    uncert_select = np.argsort(y_pred_std)\n",
    "    num_select = int(np.round(len(uncert_select) * (1.0 - select_frac)))\n",
    "    # np.argsort sorts from min to max so selecting from the end of array gives the\n",
    "    # the max uncertainty\n",
    "    return uncert_select[num_select:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_split(select_idx, X_new, y_new, X_train, y_train, X_test, y_test, seed=None):\n",
    "    X_uncert = X_new[select_idx]\n",
    "    y_uncert = y_new[select_idx]\n",
    "    X_tot = np.concatenate((X_train, X_test, X_uncert), axis=0)\n",
    "    y_tot = np.concatenate((y_train, y_test, y_uncert), axis=0)\n",
    "    # order is different than deepchem (train, train, test, test)\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X_tot, y_tot, test_size=0.2, \n",
    "                                                            random_state=seed)\n",
    "    return X1_train, y1_train, X1_test, y1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(current_dir, \"..\", \"data\", \"medium_dataset\")\n",
    "data_files = []\n",
    "for file in os.listdir(dataset_dir):\n",
    "    if file.endswith(\".sdf\"):\n",
    "        data_files.append(os.path.join(dataset_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop is variable (meaning you get a different answer each time you run it) because the splits are random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds to initialize a deterministic state, needs to be the same length as the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [5, 22, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want random seeds set all values in seeds to None or remove seed=seeds[i] from loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: random splitting changes the RMSE values a lot, \n",
    "# try stratified splitting to maintain underlying distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from /Users/bwood/google_drive/Documents/Projects/lithium_placement_ML/code/molecular_conformations_ML/notebooks/../data/medium_dataset/pe_conformer_1.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.450 s\n",
      "TIMING: dataset construction took 0.600 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.010 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.007 s\n",
      "Loading dataset from disk.\n",
      "\n",
      "\n",
      "Number of Training Data Points: 400\n",
      "Initial Train RMSE: 0.0\n",
      "Initial Test RMSE: 21.42\n",
      "\n",
      "\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from /Users/bwood/google_drive/Documents/Projects/lithium_placement_ML/code/molecular_conformations_ML/notebooks/../data/medium_dataset/pe_conformer_2.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.459 s\n",
      "TIMING: dataset construction took 0.607 s\n",
      "Loading dataset from disk.\n",
      "\n",
      "\n",
      "Number of Training Data Points: 640\n",
      "Loop 1 Train RMSE: 0.0\n",
      "Loop 1 Test RMSE: 18.82\n",
      "\n",
      "\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "Reading structures from /Users/bwood/google_drive/Documents/Projects/lithium_placement_ML/code/molecular_conformations_ML/notebooks/../data/medium_dataset/pe_conformer_3.sdf.\n",
      "Currently featurizing feature_type: CoulombMatrixEig\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.454 s\n",
      "TIMING: dataset construction took 0.593 s\n",
      "Loading dataset from disk.\n",
      "\n",
      "\n",
      "Number of Training Data Points: 880\n",
      "Loop 2 Train RMSE: 0.01\n",
      "Loop 2 Test RMSE: 13.25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpr_model = GaussianProcessRegressor(normalize_y=True)\n",
    "for i, file in enumerate(data_files):\n",
    "    # first iteration of the learning loop\n",
    "    if i == 0:\n",
    "        # load and split the initial data file\n",
    "        X_train, y_train, X_test, y_test = load_and_split(file, seed=seeds[i])\n",
    "        # initial model training and prediction\n",
    "        train_rmse, test_rmse = train_and_predict(gpr_model, X_train, y_train, X_test, y_test)\n",
    "        # print\n",
    "        print(\"\\n\")\n",
    "        print(\"Number of Training Data Points: \" + str(len(X_train)))\n",
    "        print(\"Initial Train RMSE: \".format(i=i) + str(train_rmse))\n",
    "        print(\"Initial Test RMSE: \".format(i=i) + str(test_rmse))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # all subsequent iterations of learning loop\n",
    "    else:\n",
    "        # load new data\n",
    "        X_new, y_new = load(file)\n",
    "        # predict and select the most uncertain conformers\n",
    "        select_idx = predict_and_select(gpr_model, X_new, 0.6)\n",
    "        # merge the new data into the dataset\n",
    "        X_train, y_train, X_test, y_test = merge_and_split(select_idx, X_new, y_new, \n",
    "                                                           X_train, y_train, \n",
    "                                                           X_test, y_test, seed=seeds[i])\n",
    "        # train and predict\n",
    "        train_rmse_update, test_rmse_update = train_and_predict(gpr_model, \n",
    "                                                                X_train, \n",
    "                                                                y_train, \n",
    "                                                                X_test, \n",
    "                                                                y_test)\n",
    "        # print\n",
    "        print(\"\\n\")\n",
    "        print(\"Number of Training Data Points: \" + str(len(X_train)))\n",
    "        print(\"Loop {i} Train RMSE: \".format(i=i) + str(train_rmse_update))\n",
    "        print(\"Loop {i} Test RMSE: \".format(i=i) + str(test_rmse_update))\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
